> Распределенное ПО - это ПО, состоящее из отдельных модулей (узлов), разворачиваемых на разных серверах.

## Шаблоны проектирования

### Event sourcing
> все изменения, вносимые в состояние приложения (сущности), сохраняются в той последовательности, в которой они происходили.
> 
> Эти записи служат как источником для получения текущего состояния, так и журналом аудита того, что происходило в приложении за время его существования.

#### Материалы
[Event sourcing](https://habr.com/ru/companies/otus/articles/518282/)

#### Что такое event sourcing
Обычно приложения описываются как совокупность **сущностей (entity)**, представляющих собой контейнеры для хранения состояния и **событий (event)**, отображающих изменения этих сущностей в результате обработки.
Часто события инициируется **командами (command)**, исходящими от пользователей, фоновых процессов или интеграций с внешними системами.

В традиционном видении архитектуры ПО **entity** находится в центре внимания. И все ПО пишется вокруг работы с **entity**. А события находятся "сбоку", они второстепенны.

> Event Sourcing переворачивает этот подход, фокусируясь на **событиях**, на их реализации. На том как они сохраняются и как могут быть использованы для получения состояния сущности.

#### Принцип работы event sourcing
> Пример работы с банковским счетом.

Элементы структуры:
- **entity**: банковский счет (Bank Account)
- **команды (command)**:
  - внести деньги (deposit)
  - снять деньги (withdraw)
- **события (event)**:
  - «Account Credited» (Счет пополнен)
  - «Account Debited» (Средства списаны со счета)

![[Pasted image 20240801223550.png]]

Самая простая реализация Event Sourcing требует **журнала событий (event log или event store)**, который представляет собой просто последовательность событий.

![[Pasted image 20240801223706.png]]

Текущий баланс вычисляется через обработку всех событий до текущего момента. Так как каждое событие имеет неявную метку времени, то можно вычислить состояние счета на любой момент времени, обработав все события за необходимый промежуток времени.

Event Sourcing не исключает хранилища сущностей. Часто хранилища сущностей присутствуют и в Event Sourcing — проектах.
Так будет выглядеть хранилище сущностей для нашего примера после обработки всех команд.
![[Pasted image 20240801223845.png]]


### CQRS (Command and Query Responsibility Segregation)
> это подход к проектированию программных систем, который разделяет операции изменения данных (команды) и операции чтения данных (запросы).

Основные идеи:  
- **Разделение обязанностей**: В традиционных системах команды и запросы часто обрабатываются одинаково. В CQRS они разделяются. Это позволяет оптимизировать каждую часть отдельно.  
- **Модели данных**: Для команд и запросов могут использоваться разные модели данных. Например, для записи можно использовать сложную модель с бизнес-логикой, а для чтения — более простую, оптимизированную для быстрого доступа.  
- **Упрощение масштабирования**: Поскольку команды и запросы обрабатываются отдельно, можно легче настраивать производительность. Например, можно масштабировать только часть, отвечающую за чтение. 
  
В итоге, CQRS помогает создавать более эффективные и гибкие системы, особенно когда количество операций чтения и записи значительно различается.

#### Материалы
[Хороший пример CQRS + Event sourcing](https://habr.com/ru/amp/publications/716218/)

#### Плюсы / Минусы
**+** :
- **Разделение обязанностей**: Четкое разделение команд и запросов упрощает разработку и понимание системы.
- **Оптимизация производительности**: Можно оптимизировать каждую часть отдельно, что позволяет лучше справляться с различными нагрузками.
- **Улучшенная масштабируемость**: Легко масштабировать компоненты системы в зависимости от требований, например, увеличить мощности только для запросов.

**-** :
- **Сложность**: Введение дополнительных слоев и моделей может увеличить сложность системы, что требует больше усилий на разработку и поддержку.  
- **Необходимость синхронизации**: Разделение может привести к проблемам с синхронизацией данных, так как изменения в одной модели могут не сразу отражаться в другой.

#### Examples
##### Простейший, только с реляционной БД
Например, это может быть два микросервиса:
- первый - работает только на запись данных.
- второй - только на чтение тех же данных.

При этом у каждого микросервиса может быть своя база данных. Но эти базы данных должны между собой быть синхронизированы.
А может быть и одна общая БД, с двумя доменными моделями в одной базе.

Доменная модель (то есть сущности, состав их полей и т.д.) у каждого микросервиса может быть своя:
- у первого - удобная для записи;
- у второго - данные в виде, удобном для чтения.

##### CQRS + Event sourcing
> Главным недостатком архитектуры `Event sourcing` является большой объем данных событий, которые нужно хранить.
> Данное решение необходимо использовать там, где оно уместно. А не заменять им все CRUD модели. Не везде необходимо хранить события (разные состояния объекта), часто это бесполезная и ненужная inf-я.

![[Pasted image 20240803013128.png]]
Тут:
- `Client` — клиент наших сервисов (пользовательский интерфейс, внешний/внутренний сервис).
- `Commands` — команды в определении CQRS, операции, результат которых приводит к созданию событий в Event Store.
- `Command Service A` — сервис или группа сервисов, которые обслуживают команды определенных типов. Например, сервис заказов.
- `Command Service B` — другой сервис команд.
- `Event Store` — хранилище событий. Например, MongoDB, PostgreSQL, Apache Cassandra, Redis. Главная характеристика при выборе Event Store — это производительность добавления записи.
- `Events` — события.
- `Event Bus` — шина событий. Например, Apache Kafka, Redis, RabbitMQ.
- `Query` — команда - запрос на получение агрегированных данных.
- `Query Service A` — сервис или группа сервисов, которые обслуживают запросы определенных типов. Например, история заказов.
- `Query Service B` — другой сервис запросов.
- `Query Storage` — хранилище агрегированных данных, оптимальных для запросов. Тут для двух сервисов использован один Query Storage, но вы можете для каждого сервиса запросов использовать свой storage. Например, для одного Mongodb базу, для другого PostgreSQL.

#### Область применения
CQRS отлично подходит для приложений, интенсивно использующих данные, например систем управления базами данных SQL или NoSQL. Также паттерн используется для архитектур микросервисов с большим объемом данных. Он прекрасно подходит для приложений, сохраняющих состояние благодаря разделению на писателя и читателя.

### Распределенные транзакции
> Распределенная транзакция - это когда в двух или более сетевых узлах (микросервисах) должны произойти изменения атомарно.

#### Материалы
[Статья](https://habr.com/ru/articles/769102/)

#### Модульный монолит
> В данном подходе каждый из микросервисов А и Б является отдельной библиотекой, которые устанавливаются в общее пространство и имеют доступ к одной и той же бд.
> 
> Поскольку они находятся в одном пространстве и используют одну бд, они могут выполнять обработку в рамках одной и той же транзакции.

![[Pasted image 20240803184050.png]]

Плюсы:
- простая семантика транзакций, позволяющая легко обеспечивать консистентность.

Минусы:
- единый рантайм сервисов А и Б не позволяет нам независимо деплоить и масштабировать модули, а также обеспечивать отказоустойчивость.

#### Двухфазная фиксация (2PC) - Two-Phase Commit
> используется для обеспечения согласованности данных в распределённых системах. Где необходимо гарантировать, что все узлы либо зафиксируют изменения, либо откатят их в случае неудачи.

Всем процессом управляет менеджер транзакций (координатор).

![[Pasted image 20240803191703.png]]

Алгоритм 2PC состоит из двух фаз:

**Фаза 1**: Подготовка (`Prepare`)
  1. Координатор посылает всем участникам (подчинённым) запрос о подготовке к выполнению транзакции. Этот запрос включает информацию о том, какие изменения должен выполнить каждый узел.
  2. Участники, получив запрос, выполняют необходимые локальные проверки (например, проверяют, могут ли они выполнить транзакцию на своих данных) и, если все проверки пройдены, изменяют состояние своей системы в "предварительное" состояние. Это означает, что изменения готовы к фиксации, но ещё не зафиксированы.
  3. Каждый участник отвечает координатору, сообщая о своей готовности (или неготовности) продолжить. Если все участники ответили, что они готовы, то процесс переходит ко второй фазе.  
  
**Фаза 2**: Фиксация (`Commit`)
1. После того как все участники подтвердят готовность, координатор транзакции отправляет команду «зафиксировать» (commit) всем участникам. Это значит, что все изменения теперь могут быть окончательно применены.
2. Каждый участник выполняет команду «зафиксировать», что приводит к окончательному применению всех изменений.
3. Если хотя бы один из участников сообщает о неготовности в первой фазе, координатор отправляет всем участникам команду «откатить» (rollback), что приводит к отмене всех изменений.

##### Плюсы / Минусы
**+** :
- гарантируется атомарность транзакции
- строгая консистентность данных

**-** :
- сложная конфигурация
- низкая производительность
- ограниченная масштабируемость (практически невозможно масштабировать)
- возможные отказы, при падении менеджера транзакций

#### Saga
> это асинхронный паттерн не использующий центр управления. Сервисы здесь сами взаимодействуют между собой.

Для связи между сервисами в Saga используется шина событий. Шина передаёт запросы между службами, и каждая участвующая служба создает локальную транзакцию.

##### Оркестрация Saga
В модульном монолите и 2PC мы гарантируем консистентность данных.

Но что если мы хотим ослабить требования к консистентности, при этом сохранив управление из одного места. В таком случае нам подойдет оркестрация. Когда один из сервисов выступает в качестве Оркестратора для всего состояния в распределенной системе.

Оркестратор несет ответственность за вызов сервисов, до тех пор пока они не достигнуть требуемого состояния или выполнит корректирующие действия в случае возникновения ошибки.
Оркестратор использует свою бд для хранения состояния изменений и он ответственен за восстановление при любом падении в процессе изменения состояния.

![[Pasted image 20240804192106.png]]

У нас есть сервис А, который выступает как оркестратор, он вызывает сервис Б и восстанавливается от падений с помощью компенсаторной операции при необходимости.
Ключевым моментом является то, что оба сервиса работают в своих локальных транзакциях.

Оркестрация является подходом с **консистентностью в конечном счете**, которая может включать в себя реатраи и роллбеки, для поддержки консистентного состояния.

Сервисы участники данного паттерна должны предоставлять идемпотентные операции, поскольку возможно выполнение ретраев, также они должны предоставлять эндпоинты для отката транзакций, чтобы сохранять согласованность данных.

##### Хореография SAGA
Альтернативным решением для Оркестрации является Хореография - где нет оркестратора, управляющего всем процессом.

> В данном паттерне каждый из сервисов выполняет локальную транзакцию и публикует события, которые вызывают локальные транзакции в остальных сервисах.

![[Pasted image 20240804192653.png]]

###### Хореография с двойной записью
> При использовании данного паттерна мы сталкиваемся с проблемой двойной записи, когда нам необходимо выполнить локальную транзакцию и отправить сообщение в очередь.

Здесь присутствуют те же недостатки:
- `отправить сообщение затем закоммитить` - является непрактичным решением, поскольку локальная транзакция может откатиться, а сообщение уже будет отправлено
- **закоммитить транзакцию и затем отправить сообщение** - остается возможность падения приложения после коммита транзакции и перед отправкой сообщения. Однако данное решение лучше, поскольку можно задизайнить приложение таким образом, чтобы выполнялись ретраи.

###### Хореография без двойной записи
> Одним из возможных решений является запись в рамках одной транзакции в бд и никуда не отправлять сообщение.

Пример: Сервис А сохраняет запрос и пишет в бд А. Сервис Б периодически опрашивает Сервис А и находит изменения, при обнаружении изменений Сервис Б обновляет свою бд.

![[Pasted image 20240804195128.png]]

В данном подходе сервис Б смотрит напрямую в бд А.  
Что выстраивает жесткую связность между сервисами, поскольку изменения в структуре бд на стороне сервиса А влекут за собой изменения на стороне сервиса Б.

Однако данный подход страдает еще от одного недостатка - необходимость постоянно запрашивать сервисом Б данные из сервиса А.

###### Хореография с Debezium
> Можно использовать инструмент наподобие `Debezium`, который может отследить обновления в Сервисе А, захватывая изменение данных бд (CDC - change data capture), используя транзакционный лог базы данных А.

![[Pasted image 20240804195306.png]]

Debezium умеет отслеживать транзакционный лог в ожидании необходимых событий, после чего отправлять их в Apache Kafka.

Сервис Б слушает топик в Kafka, вместо того, чтобы опрашивать сервис А.

###### Плюсы/Минусы Хореографии
**+** :
- нет единого координатора
- улучшенная масштабируемость и устойчивость

**-** :
- глобальное состояние системы распределено по сервисам-участникам (сложно узнать текущее состояние)
- согласованность в конечном счете

#### Parallel pipelines
> Данный паттерн подразумевает возможность параллельно обрабатывать запросы, когда между сервисом А и сервисом Б нет прямой зависимости.

Добавляется сервис router, который будет посылать сообщения в оба сервиса в рамках одной транзакции.

![[Pasted image 20240804201058.png]]

##### Listen to yourself
> Еще одним вариантом parellel pipelines является ситуация, когда один из сервисов сам выступает роутером.

Тогда сервис А при получении сообщения кладет его в брокер сообщений, откуда оно будет считано как сервисом А, так и сервисом Б.

![[Pasted image 20240804201153.png]]

##### Плюсы/Минусы  Parallel pipelines
**+** :
- простая, масштабируемая архитектура для параллельной обработки

**-** :
- сложно понять в каком состоянии находится система в конкретный момент времени

### Реплицированные сервисы с распределением нагрузки (RLBS)
RLBS — это самый простой и часто используемый шаблон проектирования. На базовом уровне он состоит из нескольких идентичных сервисов, которые общаются с центральным распределителем. Каждый сервис способен выполнять задачи и перезапускать их в случае неудачи. Распределитель получает запросы от конечного пользователя и разделяет их между сервисами, используя round-robin или более сложный алгоритм.

Дублирующие службы обеспечивают высокую доступность приложения для запросов пользователей и могут перераспределять работу в случае сбоя одного экземпляра службы.

#### Плюсы/Минусы
**+** :
- Стабильная производительность с точки зрения конечного пользователя.
- Быстрое восстановление после сбоев.
- Высокая масштабируемость через увеличение числа сервисов.

**-** :
- Производительность, зависит от алгоритма балансировки.
- Управление сервисами требует много ресурсов.

### Шардинг
 Альтернативой репликации является создание отдельных сервисов, каждый из которых выполняет определённый тип запросов. Это называется шардингом, потому что вы разделяете запрос на несколько неодинаковых частей. Например, у вас может быть отдельный сервис, который принимает кэширующиеся запросы, а другой сервис будет отвечать за запросы с высоким приоритетом. Распределитель нагрузки обрабатывает каждый запрос и передаёт его в подходящий сервис.